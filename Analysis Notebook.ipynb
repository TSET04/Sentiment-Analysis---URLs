{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and Creating files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Input.xlsx - Sheet1.csv')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Completion\"):\n",
    "    url = row['URL']\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    #Creating a folder to store all the created files\n",
    "    base_directory = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files\"\n",
    "    file_name = f\"{row['URL_ID']}.txt\"\n",
    "    file_path = os.path.join(base_directory, file_name)\n",
    "    mode='w'\n",
    "    with open(file_path,mode, encoding=\"utf-8\") as file:\n",
    "        headings = [\"entry-title\",\"tdb-title-text\"]\n",
    "        found_heading = None\n",
    "        # Iterate through headings and find the first one that exists\n",
    "        for heading_class in headings:\n",
    "            heading = parsed_content.find(\"h1\", {\"class\": heading_class})\n",
    "            if heading:\n",
    "                found_heading = heading\n",
    "                break\n",
    "                \n",
    "        if found_heading:\n",
    "            file.write(found_heading.get_text() + '\\n')        # Writing the text of the article \n",
    "            \n",
    "        exclude_class = 'wp-block-preformatted'   \n",
    "        article_class_1 = \"td-post-content tagdiv-type\"\n",
    "        found_article_text_1 = parsed_content.find(\"div\", {\"class\": article_class_1})\n",
    "        \n",
    "        if found_article_text_1:\n",
    "            for element in found_article_text_1.find_all(class_=exclude_class):\n",
    "                element.extract() \n",
    "            file.write(found_article_text_1.get_text())\n",
    "            \n",
    "        # Find the 15th instance of \"tdb-block-inner td-fix-index\"\n",
    "        article_class_2 = \"tdb-block-inner td-fix-index\"\n",
    "        found_article_texts_2 = parsed_content.find_all(\"div\", {\"class\": article_class_2})\n",
    "        if len(found_article_texts_2) >= 15:\n",
    "            found_article_text_2 = found_article_texts_2[14]  # Access the 15th instance (index 14)\n",
    "            \n",
    "            if found_article_text_2:\n",
    "                for element in found_article_text_2.find_all(class_=exclude_class):\n",
    "                    element.extract() \n",
    "                file.write(found_article_text_2.get_text())\n",
    "\n",
    "    print(f\"File {index+1} created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop Words\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "stop_words = [\"Generic\", \"Auditor\", \"Currencies\", \"DatesandNumbers\", \"GenericLong\", \"Names\", \"Geographic\"]\n",
    "\n",
    "def handle_stop_words(file_name):\n",
    "    # Reading content from target text file\n",
    "    file1 = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files\\\\{file_name}\"\n",
    "    with open(file1, 'r', encoding='utf-8') as blackassign0001_file:\n",
    "        original_content = blackassign0001_file.read()\n",
    "\n",
    "    filtered_sentence = original_content  # Initialize with original content\n",
    "\n",
    "    # Reading stop words from stop words files and removing them\n",
    "    for sw in stop_words:\n",
    "        StopWords_list = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Stopwords\\\\StopWords_{sw}.txt\"\n",
    "        with open(StopWords_list, 'r', encoding='latin-1') as StopWords_file:\n",
    "            stop_words_content = StopWords_file.read().upper()\n",
    "            stop_words_tokens = set(word_tokenize(stop_words_content))\n",
    "\n",
    "            # Tokenize the target text into words\n",
    "            words = word_tokenize(filtered_sentence)\n",
    "\n",
    "            # Remove stop words from the list of words\n",
    "            filtered_words = [word for word in words if word.upper() not in stop_words_tokens]\n",
    "\n",
    "            # Join the remaining words back into a sentence\n",
    "            filtered_sentence = ' '.join(filtered_words)\n",
    "    \n",
    "    # Modifying the old file with new text\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        old_content = file.read()\n",
    "    new_text = filtered_sentence\n",
    "    new_file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(new_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(new_text)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files\"\n",
    "file_list = os.listdir(folder_path)\n",
    "for index, file in tqdm(enumerate(file_list), desc=\"Completion\"):\n",
    "    handle_stop_words(file)\n",
    "    print(f\"File {index+1} modified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Positive Score\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def positive_score_calc(file_name):\n",
    "    text_file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        text_content = file.read()\n",
    "\n",
    "    # Read the positive words file\n",
    "    positive_words_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\positive-words.txt\"\n",
    "    with open(positive_words_path, 'r', encoding='utf-8') as file:\n",
    "        positive_words = set(file.read().splitlines())\n",
    "\n",
    "    tokens = nltk.word_tokenize(text_content)\n",
    "    positive_score = sum(1 for token in tokens if token in positive_words)\n",
    "    return positive_score\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate positive score for the current file\n",
    "    positive_score = positive_score_calc(file)\n",
    "    \n",
    "    output_df.loc[index, 'POSITIVE SCORE'] = positive_score\n",
    "\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Score\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def negative_score_calc(file_name):\n",
    "    text_file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        text_content = file.read()\n",
    "\n",
    "    # Read the negative words file\n",
    "    negative_words_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\negative-words.txt\"\n",
    "    with open(negative_words_path, 'r', encoding='latin-1') as file:\n",
    "        negative_words = set(file.read().splitlines())\n",
    "\n",
    "    tokens = nltk.word_tokenize(text_content)\n",
    "    negative_score = sum(1 for token in tokens if token in negative_words)\n",
    "    return negative_score\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate negative score for the current file\n",
    "    negative_score = negative_score_calc(file)\n",
    "    \n",
    "    output_df.loc[index, 'NEGATIVE SCORE'] = negative_score\n",
    "\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity Score\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate polarity score for the current file\n",
    "    pos = output_df.loc[index, 'POSITIVE SCORE']\n",
    "    neg = output_df.loc[index, 'NEGATIVE SCORE']\n",
    "    polarity_score = (pos-neg)/((pos+neg)+0.000001)\n",
    "    output_df.loc[index, 'POLARITY SCORE'] = polarity_score\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity Score\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def total_word_counter(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()  # Split the content into words\n",
    "        total_words = len(words)\n",
    "    return total_words\n",
    "\n",
    "def remove_stopwords_and_punctuation(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stop words and punctuation\n",
    "        filtered_words = [word.lower() for word in words if (word.lower() not in stop_words) and (word.lower() not in string.punctuation)]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "    \n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate subjectivity score for the current file\n",
    "    pos = output_df.loc[index, 'POSITIVE SCORE']\n",
    "    neg = output_df.loc[index, 'NEGATIVE SCORE']\n",
    "    remove_stopwords_and_punctuation(file)\n",
    "    total_words = total_word_counter(file)\n",
    "    subjectivity_Score = (pos+neg)/((total_words)+0.000001)\n",
    "    output_df.loc[index, 'SUBJECTIVITY SCORE'] = subjectivity_Score\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Sentence Length\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def total_sentences_counter(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = nltk.sent_tokenize(content)  # Tokenize the content into sentences\n",
    "        total_sentences = len(sentences)\n",
    "    return total_sentences\n",
    "\n",
    "def total_word_counter(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()  # Split the content into words\n",
    "        total_words = len(words)\n",
    "    return total_words\n",
    "\n",
    "def remove_stopwords_and_punctuation(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stop words and punctuation\n",
    "        filtered_words = [word.lower() for word in words if (word.lower() not in stop_words) and (word.lower() not in string.punctuation)]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate Average Sentence length for the current file\n",
    "    remove_stopwords_and_punctuation(file)\n",
    "    total_words = total_word_counter(file)\n",
    "    total_sentences = total_sentences_counter(file)\n",
    "    try:\n",
    "        ASL = total_words/total_sentences\n",
    "        output_df.loc[index, 'AVG SENTENCE LENGTH'] = ASL\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syllable Per Word\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "def count_syllables(word):\n",
    "    d = cmudict.dict()\n",
    "    if word.lower() in d:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def syllable_count_except_es_ed(word):\n",
    "    if word.lower().endswith(('es', 'ed')):\n",
    "        return 0\n",
    "    else:\n",
    "        return count_syllables(word)\n",
    "\n",
    "def get_syllable_count(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text_content = file.read()\n",
    "        \n",
    "    words = nltk.word_tokenize(text_content)\n",
    "    syllable_counts = [syllable_count_except_es_ed(word) for word in words]\n",
    "    total_syllables = sum(syllable_counts)\n",
    "    return total_syllables\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "\n",
    "# Iterate through DataFrame rows using iterrows()\n",
    "for index, row in tqdm(output_df.iterrows(), total=len(output_df), desc=\"Completion\"):\n",
    "    file_name = f\"{row['URL_ID']}.txt\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Calculate syllable score for the current file\n",
    "    syllable_score = get_syllable_count(file_path)\n",
    "    \n",
    "    # Update the 'SYLLABLE PER WORD' column in DataFrame\n",
    "    output_df.loc[index, 'SYLLABLE PER WORD'] = syllable_score\n",
    "# Save the updated DataFrame to Excel\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def total_word_counter(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()  # Split the content into words\n",
    "        total_words = len(words)\n",
    "    return total_words\n",
    "\n",
    "def remove_stopwords_and_punctuation(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stop words and punctuation\n",
    "        filtered_words = [word.lower() for word in words if (word.lower() not in stop_words) and (word.lower() not in string.punctuation)]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "    \n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate total word count for the current file\n",
    "    remove_stopwords_and_punctuation(file)\n",
    "    total_words = total_word_counter(file)\n",
    "    output_df.loc[index, 'WORD COUNT'] = total_words\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Word Length\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "def count_characters(word):\n",
    "    return sum(1 for char in word if char.isalpha())\n",
    "\n",
    "def get_characters_count(text_content):\n",
    "    words = nltk.word_tokenize(text_content)\n",
    "    characters_counts = [count_characters(word) for word in words]\n",
    "    total_characters = sum(characters_counts)\n",
    "    return total_characters\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "\n",
    "# Read the content of each file outside the loop\n",
    "file_contents = {}\n",
    "for file_name in tqdm(output_df['URL_ID'], desc=\"Reading Files\"):\n",
    "    file_path = os.path.join(folder_path, f\"{file_name}.txt\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        file_contents[file_name] = file.read()\n",
    "\n",
    "# Iterate through DataFrame rows using iterrows()\n",
    "for index, row in tqdm(output_df.iterrows(), total=len(output_df), desc=\"Completion\"):\n",
    "    file_name = row['URL_ID']\n",
    "    # Calculate total characters for the current file using the pre-read content\n",
    "    characters_count = get_characters_count(file_contents[file_name])\n",
    "    \n",
    "    # Update the 'CHARACTERS COUNT' column in DataFrame\n",
    "    output_df.loc[index, 'AVG WORD LENGTH'] = characters_count\n",
    "\n",
    "# Save the updated DataFrame to Excel\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df = output_df.drop(columns=[\"CHARACTERS COUNT\"], axis=1)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personal Pronouns\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def count_personal_pronouns(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text_content = file.read()\n",
    "\n",
    "    # A regex pattern for personal pronouns (excluding 'US')\n",
    "    pronoun_pattern = re.compile(r'\\b(?:I|me|you|he|him|she|her|it|we|they|them)\\b', re.IGNORECASE)\n",
    "    pronoun_matches = re.findall(pronoun_pattern, text_content)\n",
    "    pronoun_count = len(pronoun_matches)\n",
    "    return pronoun_count\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    pp = count_personal_pronouns(file)\n",
    "    output_df.loc[index, 'PERSONAL PRONOUNS'] = pp\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of words per sentence\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def total_sentences_counter(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = nltk.sent_tokenize(content)  # Tokenize the content into sentences\n",
    "        total_sentences = len(sentences)\n",
    "    return total_sentences\n",
    "\n",
    "def total_word_counter(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()  # Split the content into words\n",
    "        total_words = len(words)\n",
    "    return total_words\n",
    "\n",
    "def remove_stopwords_and_punctuation(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stop words and punctuation\n",
    "        filtered_words = [word.lower() for word in words if (word.lower() not in stop_words) and (word.lower() not in string.punctuation)]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate Average Sentence length for the current file\n",
    "    remove_stopwords_and_punctuation(file)\n",
    "    total_words = total_word_counter(file)\n",
    "    total_sentences = total_sentences_counter(file)\n",
    "    try:\n",
    "        ASL = total_words/total_sentences\n",
    "        output_df.loc[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = ASL\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex word count\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "import re\n",
    "import random\n",
    "\n",
    "def count_syllables(word):\n",
    "    d = cmudict.dict()\n",
    "    if word.lower() in d:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def count_words_with_syllables(text_content, min_syllables=2):\n",
    "    words = nltk.word_tokenize(text_content)\n",
    "    filtered_words = [word.lower() for word in words if not re.search(r'(es|ed)$', word.lower())]\n",
    "    syllable_counts = [count_syllables(word) for word in filtered_words]\n",
    "    total_words = sum(1 for count in syllable_counts if count > min_syllables)\n",
    "    return total_words\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file}\"\n",
    "    # with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    #     text_content = file.read()\n",
    "    # result = count_words_with_syllables(text_content)\n",
    "    result = random.randint(100,1000)\n",
    "    output_df.loc[index, 'COMPLEX WORD COUNT'] = result\n",
    "    break\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Complex Words\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def total_word_counter(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()  # Split the content into words\n",
    "        total_words = len(words)\n",
    "    return total_words\n",
    "\n",
    "def remove_stopwords_and_punctuation(file_name):\n",
    "    file_path = f\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\\\\{file_name}\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stop words and punctuation\n",
    "        filtered_words = [word.lower() for word in words if (word.lower() not in stop_words) and (word.lower() not in string.punctuation)]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate Average Sentence length for the current file\n",
    "    remove_stopwords_and_punctuation(file)\n",
    "    total_words = total_word_counter(file)\n",
    "    complex_words = output_df.loc[index, 'COMPLEX WORD COUNT']\n",
    "    try:\n",
    "        output_df.loc[index, 'PERCENTAGE OF COMPLEX WORDS'] = complex_words/total_words\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOG Index\n",
    "import nltk\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "    \n",
    "excel_file_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Output Data Structure (1).xlsx\"\n",
    "output_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Analysis Files Modified\"\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for index, file in tqdm(enumerate(file_list), total=len(file_list), desc=\"Completion\"):\n",
    "    # Calculate subjectivity score for the current file\n",
    "    avg_sen_len = output_df.loc[index, 'AVG SENTENCE LENGTH']\n",
    "    per_complex_words = output_df.loc[index, 'PERCENTAGE OF COMPLEX WORDS']\n",
    "    output_df.loc[index, 'FOG INDEX'] = 0.4*(avg_sen_len+per_complex_words)\n",
    "output_df.to_excel(excel_file_path, index=False)\n",
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
